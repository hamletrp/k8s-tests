
associate-iam-oidc-provider
    eksctl utils associate-iam-oidc-provider --region us-east-1  --cluster cluster-lab3 --approve


get oicd provider 
    aws eks describe-cluster --region us-east-1 --name cluster-lab3 --query "cluster.identity.oidc.issuer" --output text

Role setup
    thru terraform
    or
    1: 
        aws iam create-policy \
        --policy-name ClusterAutoscalerPolicy \
        --policy-document file://autoscaler-policy.json

    2: 
        aws iam create-role \
        --role-name ClusterAutoscalerRole \
        --assume-role-policy-document file://autoscaler-trust-policy.json

    3:
        aws iam attach-role-policy \
        --policy-arn arn:aws:iam::722249351142:policy/ClusterAutoscalerPolicy \
        --role-name ClusterAutoscalerRole

ServiceAccount setup 
    set on the CA repo via helm vars, so to set the aws role arn 
    or
    1: k apply -f ./svc-acc.yml

Tag / Label Node Groups Properly - done automatically from eksctl-cluster-config.yaml
    1: node group
        k8s.io/cluster-autoscaler/enabled = true
        k8s.io/cluster-autoscaler/<your-cluster-name> = owned
    2: nodes 
        k label nodes ip-192-168-21-14.ec2.internal cluster-autoscaler-enabled=true &&
        k label nodes ip-192-168-41-254.ec2.internal cluster-autoscaler-enabled=true

ArgoCD install 
    https://github.com/devopsjourney1/argo-examples/blob/master/readme.md
        kubectl create namespace argocd
        kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
        k get services -n argocd
        kubectl port-forward service/argocd-server -n argocd 8080:443
        kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
        
        log int argo cli 
            argocd login localhost:9090 --username admin --password EVRTZ2JBrEm1F2vy --insecure

        *Deployed on EKS required to expose the service:
            - changing the type to LoadBalancer
            - Option 2: Expose Using Ingress + AWS ALB (Production-Grade)
            - or port fowarding

            
Deploy Cluster Autoscaler
    or
        k apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/cluster-autoscaler-1.28.0/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
        k delete -f https://raw.githubusercontent.com/kubernetes/autoscaler/cluster-autoscaler-1.28.0/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
    or from a helm chart thru argo
        https://github.com/hamletrp/k8s-tests.git
            argocd login localhost:8080 --username admin --password qM9eJsCZgkNckG4m --insecure
            values.yaml
                clusterSvcAccRoleArn
                clusterName
            k apply -f ./helm-chart/k8sca-stack-app.yaml

        stress test 
            k create deploy stress --image=busybox --replicas=50 -- sleep 3600
            k scale deploy stress --replicas 20
            k delete deploy stress
        
        nodes should scale up and down
