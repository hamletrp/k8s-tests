
provission cluster 
    node group tags 
        k8s.io/cluster-autoscaler/enabled = true
        k8s.io/cluster-autoscaler/<your-cluster-name> = owned
    nodes labels 
        cluster-autoscaler-enabled=true

    eksctl create cluster -f ./eksctl-cluster-config.yml

associate-iam-oidc-provider
    eksctl utils associate-iam-oidc-provider --region us-east-1  --cluster cluster-lab3 --approve

get oicd provider 
    aws eks describe-cluster --region us-east-1 --name cluster-lab3 --query "cluster.identity.oidc.issuer" --output text

provission Role
    thru terraform
        update .tfvars file with the oidc-provider id
        terraform apply --auto-approve
    or
    1: 
        aws iam create-policy \
        --policy-name ClusterAutoscalerPolicy \
        --policy-document file://autoscaler-policy.json

    2: 
        aws iam create-role \
        --role-name ClusterAutoscalerRole \
        --assume-role-policy-document file://autoscaler-trust-policy.json

    3:
        aws iam attach-role-policy \
        --policy-arn arn:aws:iam::722249351142:policy/ClusterAutoscalerPolicy \
        --role-name ClusterAutoscalerRole

provision storage classes 
    k apply -f ebs-storageclass.yml

ArgoCD install 
    https://github.com/devopsjourney1/argo-examples/blob/master/readme.md
        kubectl create namespace argocd
        kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
        k get services -n argocd
        kubectl port-forward service/argocd-server -n argocd 9090:443
        kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
        
        log int argo cli 
            argocd login localhost:9090 --username admin --password JYM0o6YWOkZe9u0y --insecure

        *Deployed on EKS required to expose the service:
            - changing the type to LoadBalancer
            - Option 2: Expose Using Ingress + AWS ALB (Production-Grade)
            - or port fowarding

            
Deploy Cluster Autoscaler
    ServiceAccount setup 
        set clusterSvcAccRoleArn var in ca values.yaml file




    or
        k apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/cluster-autoscaler-1.28.0/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
        k delete -f https://raw.githubusercontent.com/kubernetes/autoscaler/cluster-autoscaler-1.28.0/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
    or from a helm chart thru argo
        https://github.com/hamletrp/k8s-tests.git
            argocd login localhost:8080 --username admin --password qM9eJsCZgkNckG4m --insecure
            values.yaml
                clusterSvcAccRoleArn
                clusterName
            k apply -f ./helm-chart/k8sca-stack-app.yaml

        stress test 
            k create deploy stress --image=busybox --replicas=300 -- sleep 3600
            k scale deploy stress --replicas 20
            k delete deploy stress
        
        nodes should scale up and down
